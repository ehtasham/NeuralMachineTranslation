{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machinetranslation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO9DvqOZcy6QESlXB52XRNV"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjmrpjPNl19f",
        "colab_type": "text"
      },
      "source": [
        "## Machine Translation Project\n",
        "\n",
        "## Introduction\n",
        "In this notebook, we will build a deep neural network that functions as part of an end-to-end machine translation pipeline. The completed pipeline will accept English text as input and return the French translation.\n",
        "\n",
        "- **Preprocess** - convert text to sequence of integers.\n",
        "- **Models** - Create models which accepts a sequence of integers as input and returns a probability distribution over possible translations.\n",
        "- **Prediction** - Run the model on English text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNx0lmL-VVzr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#imports\n",
        "import os\n",
        "import collections\n",
        "import numpy as np\n",
        "import project_tests as tests\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model\n",
        "from keras.layers import GRU, LSTM, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional ,Dropout\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import sparse_categorical_crossentropy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yutycSqvU8rp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load dataset\n",
        "def load_data(path):\n",
        "    input_file = os.path.join(path)\n",
        "    with open(input_file, \"r\") as f:\n",
        "        data = f.read()\n",
        "\n",
        "    return data.split('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0ATaFEMWldc",
        "colab_type": "text"
      },
      "source": [
        "### Load Data\n",
        "The data is located in `data/small_vocab_en` and `data/small_vocab_fr`. The `small_vocab_en` file contains English sentences with their French translations in the `small_vocab_fr` file. Load the English and French data from these files from running the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtsTdyskWXCl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load English Data\n",
        "english_sentences = load_data('data/small_vocab_en')\n",
        "#load French Data\n",
        "french_sentences = load_data('data/small_vocab_fr')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQo93qntXrgu",
        "colab_type": "text"
      },
      "source": [
        "### Files\n",
        "Each line in `small_vocab_en` contains an English sentence with the respective translation in each line of `small_vocab_fr`.  View the first two lines from each file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBjVYSj3Xr3c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "0e61ce9d-04af-41be-e375-f942062ae48c"
      },
      "source": [
        "#visualize data\n",
        "for text in range(2):\n",
        "  print('Englist text {}: {}'.format(text+1, english_sentences[text]))\n",
        "  print('French text {}: {}'.format(text+1, french_sentences[text]))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Englist text 1: new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
            "French text 1: new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
            "Englist text 2: the united states is usually chilly during july , and it is usually freezing in november .\n",
            "French text 2: les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vklEGUlYLrz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "7421045d-9fee-4da0-da68-e7f384dbf296"
      },
      "source": [
        "#total no of unique words in each vocabualary\n",
        "english_word_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n",
        "french_word_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])\n",
        "\n",
        "print('{} English word.'.format(len([word for sentence in english_sentences for word in sentence.split()])))\n",
        "print('{} unique Englist words.'.format(len(english_word_counter)))\n",
        "print('10 Most common words in English dataset:')\n",
        "print('\"' + '\" \"'. join(list(zip(*english_word_counter.most_common(10)))[0])+'\"')\n",
        "print(\"\")\n",
        "print('{} French word.'.format(len([word for sentence in french_sentences for word in sentence.split()])))\n",
        "print('{} unique French words.'.format(len(french_word_counter)))\n",
        "print('10 Most common words in French dataset:')\n",
        "print('\"' + '\" \"'. join(list(zip(*french_word_counter.most_common(10)))[0])+'\"')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1823250 English word.\n",
            "227 unique Englist words.\n",
            "10 Most common words in English dataset:\n",
            "\"is\" \",\" \".\" \"in\" \"it\" \"during\" \"the\" \"but\" \"and\" \"sometimes\"\n",
            "\n",
            "1961295 French word.\n",
            "355 unique French words.\n",
            "10 Most common words in French dataset:\n",
            "\"est\" \".\" \",\" \"en\" \"il\" \"les\" \"mais\" \"et\" \"la\" \"parfois\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FInHd7JKdW0M",
        "colab_type": "text"
      },
      "source": [
        "## Preprocess\n",
        "we need to convert the text into sequences of integers using the following preprocess methods:\n",
        "1. Tokenize the words into ids\n",
        "2. Add padding to make all the sequences the same length.\n",
        "\n",
        "### Tokenize\n",
        "we will Turn each sentence into a sequence of words ids using Keras's [`Tokenizer`](https://keras.io/preprocessing/text/#tokenizer) function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avtxBCNZZ-hH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "b5655bc7-f874-4c29-be92-117915fbeb76"
      },
      "source": [
        "def tokenize(sentences):\n",
        "  \"\"\"\n",
        "  Tokenize sentences\n",
        "  :param sentence: List of sentences to be tokenized\n",
        "  :return: Tuple of tokenized sentences and tokenizer\n",
        "  \"\"\"\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(sentences)\n",
        "  text_tokenized = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "  return text_tokenized, tokenizer\n",
        "\n",
        "tests.test_tokenize(tokenize)\n",
        "\n",
        "# Tokenize Example output\n",
        "text_sentences = [\n",
        "    'The quick brown fox jumps over the lazy dog .',\n",
        "    'By Jove , my quick study of lexicography won a prize .',\n",
        "    'This is a short sentence .']\n",
        "text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
        "print(text_tokenizer.word_index)\n",
        "print()\n",
        "for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
        "    print('Sequence {} in x'.format(sample_i + 1))\n",
        "    print('  Input:  {}'.format(sent))\n",
        "    print('  Output: {}'.format(token_sent))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'the': 1, 'quick': 2, 'a': 3, 'brown': 4, 'fox': 5, 'jumps': 6, 'over': 7, 'lazy': 8, 'dog': 9, 'by': 10, 'jove': 11, 'my': 12, 'study': 13, 'of': 14, 'lexicography': 15, 'won': 16, 'prize': 17, 'this': 18, 'is': 19, 'short': 20, 'sentence': 21}\n",
            "\n",
            "Sequence 1 in x\n",
            "  Input:  The quick brown fox jumps over the lazy dog .\n",
            "  Output: [1, 2, 4, 5, 6, 7, 1, 8, 9]\n",
            "Sequence 2 in x\n",
            "  Input:  By Jove , my quick study of lexicography won a prize .\n",
            "  Output: [10, 11, 12, 2, 13, 14, 15, 16, 3, 17]\n",
            "Sequence 3 in x\n",
            "  Input:  This is a short sentence .\n",
            "  Output: [18, 19, 3, 20, 21]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfC2YFUUkF3K",
        "colab_type": "text"
      },
      "source": [
        "### Padding\n",
        "we need to make sure that all the sequences are of same length by adding padding to the end of each sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiuwmGraiSl4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "ec8e175c-3e34-45f7-883e-f1ea5831875d"
      },
      "source": [
        "def pad(sentences, length=None):\n",
        "  \"\"\"\n",
        "  pad sentences\n",
        "  :param sentences: List of sentences\n",
        "  :parem length: Lenght to pad the sequence to. if None use the length of the longest sequence\n",
        "  :return: padded numpy array of sequences\n",
        "  \"\"\"\n",
        "  if length is None:\n",
        "    length=max([len(sentence) for sentence in sentences])\n",
        "\n",
        "  return pad_sequences(sentences, maxlen=length, padding='post')\n",
        "\n",
        "tests.test_pad(pad)\n",
        "\n",
        "#pad Tokenized output\n",
        "test_pad = pad(text_tokenized)\n",
        "for text, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
        "  print('Sequence {} in x'.format(text+1))\n",
        "  print('   Input: {}'.format(np.array(token_sent)))\n",
        "  print('   Output: {}'.format(pad_sent))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequence 1 in x\n",
            "   Input: [1 2 4 5 6 7 1 8 9]\n",
            "   Output: [1 2 4 5 6 7 1 8 9 0]\n",
            "Sequence 2 in x\n",
            "   Input: [10 11 12  2 13 14 15 16  3 17]\n",
            "   Output: [10 11 12  2 13 14 15 16  3 17]\n",
            "Sequence 3 in x\n",
            "   Input: [18 19  3 20 21]\n",
            "   Output: [18 19  3 20 21  0  0  0  0  0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTePUu63xbr1",
        "colab_type": "text"
      },
      "source": [
        "### Preprocess Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9FMhfW7mGt3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "9ebe52f2-896a-4db6-c2b0-1b523e7af53c"
      },
      "source": [
        "def preprocess(x, y):\n",
        "  \"\"\"\n",
        "  preprocess x and y\n",
        "  :param x: Feature List of sentences\n",
        "  :param y: Label List of sentences\n",
        "  :return: Tuple of (Preprocessed x, Preprocessed y, x_tokenizer, y_tokenizer)\n",
        "  \"\"\"\n",
        "  preprocess_x, x_tokenizer = tokenize(x)\n",
        "  preprocess_y, y_tokenizer = tokenize(y)\n",
        "\n",
        "  preprocess_x = pad(preprocess_x)\n",
        "  preprocess_y = pad(preprocess_y)\n",
        "\n",
        "  #keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
        "  preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
        "\n",
        "  return preprocess_x, preprocess_y, x_tokenizer, y_tokenizer\n",
        "\n",
        "preprocessed_english_sentences, preprocessed_french_sentences, english_tokenizer, french_tokenizer =\\\n",
        "  preprocess(english_sentences, french_sentences)\n",
        "\n",
        "\n",
        "max_english_sequence_length = preprocessed_english_sentences.shape[1]\n",
        "max_french_sequence_length = preprocessed_french_sentences.shape[1]\n",
        "english_vocab_size = len(english_tokenizer.word_index)\n",
        "french_vocab_size = len(french_tokenizer.word_index)\n",
        "\n",
        "print('Data Preprocessed')\n",
        "print(\"Max English sentence length:\", max_english_sequence_length)\n",
        "print(\"Max French sentence length:\", max_french_sequence_length)\n",
        "print(\"English vocabulary size:\", english_vocab_size)\n",
        "print(\"French vocabulary size:\", french_vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Preprocessed\n",
            "Max English sentence length: 15\n",
            "Max French sentence length: 21\n",
            "English vocabulary size: 199\n",
            "French vocabulary size: 344\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Sy7NVEpzgeG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ae8251c7-439f-4953-d6d8-1296ef72847c"
      },
      "source": [
        "def logits_to_text(logits, tokenizer):\n",
        "  \"\"\"\n",
        "  Turn logits from Neural Network into text using the tokenizer\n",
        "  :param logits: Logits from Neural Network\n",
        "  :param tokenizer: keras Tokenizer fit on the labels\n",
        "  :return: String that represents the text of logits\n",
        "  \"\"\"\n",
        "  index_to_words = {id:word for word, id in tokenizer.word_index.items()}\n",
        "  index_to_words[0]='<PAD>'\n",
        "\n",
        "  return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
        "\n",
        "  \n",
        "print('`logits_to_text` function loaded.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`logits_to_text` function loaded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrTJAl20MQqL",
        "colab_type": "text"
      },
      "source": [
        "### Model 1: RNN\n",
        "A basic RNN model is a good baseline for sequence data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3od-sM9lWf2k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "9b6bd082-8717-461d-9c78-81aef451d523"
      },
      "source": [
        "def simple_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "    \"\"\"\n",
        "    Build and train a basic RNN on x and y\n",
        "    :param input_shape: Tuple of input shape\n",
        "    :param output_sequence_length: Length of output sequence\n",
        "    :param english_vocab_size: Number of unique English words in the dataset\n",
        "    :param french_vocab_size: Number of unique French words in the dataset\n",
        "    :return: Keras model built, but not trained\n",
        "    \"\"\"\n",
        "   \n",
        "    # TODO: Build the layers\n",
        "    learning_rate = 1e-2\n",
        "    input_seq = Input(input_shape[1:])\n",
        "    rnn = LSTM(64, return_sequences=True)(input_seq)\n",
        "    dropout = Dropout(0.2)(rnn)\n",
        "    # we change `plaintext_vocab_size` to `french_vocab_size` as our ouput is now French sentences intead of plain words\n",
        "    logits = TimeDistributed(Dense(french_vocab_size))(dropout)\n",
        "\n",
        "    model = Model(input_seq, Activation('softmax')(logits))\n",
        "    model.compile(loss=sparse_categorical_crossentropy,\n",
        "                  optimizer=Adam(learning_rate),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    \n",
        "    return model\n",
        "\n",
        "# tests.test_simple_model(simple_model)\n",
        "\n",
        "\n",
        "#reshape input\n",
        "tmp_x = pad(preprocessed_english_sentences, max_french_sequence_length)\n",
        "tmp_x = tmp_x.reshape((-1, preprocessed_french_sentences.shape[-2], 1))\n",
        "\n",
        "#Train the neural network\n",
        "simple_rnn_model = simple_model(\n",
        "    tmp_x.shape,\n",
        "    max_french_sequence_length,\n",
        "    english_vocab_size,\n",
        "    french_vocab_size\n",
        ")\n",
        "\n",
        "simple_rnn_model.fit(tmp_x, preprocessed_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)\n",
        "\n",
        "#print predictions\n",
        "print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 110288 samples, validate on 27573 samples\n",
            "Epoch 1/10\n",
            "110288/110288 [==============================] - 8s 74us/step - loss: 2.3451 - accuracy: 0.4917 - val_loss: nan - val_accuracy: 0.5742\n",
            "Epoch 2/10\n",
            "110288/110288 [==============================] - 6s 57us/step - loss: 1.6524 - accuracy: 0.5791 - val_loss: nan - val_accuracy: 0.6168\n",
            "Epoch 3/10\n",
            "110288/110288 [==============================] - 6s 58us/step - loss: 1.4886 - accuracy: 0.6039 - val_loss: nan - val_accuracy: 0.6284\n",
            "Epoch 4/10\n",
            "110288/110288 [==============================] - 6s 56us/step - loss: 1.3969 - accuracy: 0.6177 - val_loss: nan - val_accuracy: 0.6447\n",
            "Epoch 5/10\n",
            "110288/110288 [==============================] - 6s 54us/step - loss: 1.3360 - accuracy: 0.6271 - val_loss: nan - val_accuracy: 0.6502\n",
            "Epoch 6/10\n",
            "110288/110288 [==============================] - 6s 55us/step - loss: 1.2968 - accuracy: 0.6327 - val_loss: nan - val_accuracy: 0.6543\n",
            "Epoch 7/10\n",
            "110288/110288 [==============================] - 6s 55us/step - loss: 1.2644 - accuracy: 0.6377 - val_loss: nan - val_accuracy: 0.6539\n",
            "Epoch 8/10\n",
            "110288/110288 [==============================] - 6s 55us/step - loss: 1.2392 - accuracy: 0.6418 - val_loss: nan - val_accuracy: 0.6624\n",
            "Epoch 9/10\n",
            "110288/110288 [==============================] - 6s 54us/step - loss: 1.2182 - accuracy: 0.6455 - val_loss: nan - val_accuracy: 0.6697\n",
            "Epoch 10/10\n",
            "110288/110288 [==============================] - 6s 54us/step - loss: 1.1992 - accuracy: 0.6500 - val_loss: nan - val_accuracy: 0.6748\n",
            "new jersey est parfois calme en l' et il est il est en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJvUHutCXrE6",
        "colab_type": "text"
      },
      "source": [
        "### Model 2: Embedding (IMPLEMENTATION)\n",
        "In the simple model we have used ids to represent words, but there's a better representation of a word. This is called word embeddings. An embedding is a vector representation of the word that is close to similar words in n-dimensional space, where the n represents the size of the embedding vectors.\n",
        "\n",
        "In this model, we willcreate a RNN model using embedding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swJdSWm58HrZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0b28dc9d-1d91-474c-9c59-f9fcb052e8bd"
      },
      "source": [
        "def embedding_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "  \"\"\"\n",
        "  Build and train a RNN model using word embedding on x and y\n",
        "  :param input_shape: Tuple of input shape\n",
        "  :param output_sequence_length: length og output sequence\n",
        "  :param english_vocab_size: Number of unique English words in the dataset\n",
        "  :param french_vocab_size: Number of unique French words in dataset\n",
        "  :return: keras model built, but not trained\n",
        "  \"\"\"\n",
        "  \n",
        "  learning_rate = 1e-2\n",
        "  \n",
        "  input_seq = Input(input_shape[1:])\n",
        "  \n",
        "  embed_layer = Embedding(english_vocab_size, 64, input_length=output_sequence_length)(input_seq)\n",
        "\n",
        "  rnn = LSTM(64, return_sequences=True)(embed_layer)\n",
        "\n",
        "  dropout = Dropout(0.2)(rnn)\n",
        "\n",
        "  logits = TimeDistributed(Dense(french_vocab_size))(dropout)\n",
        "\n",
        "  model = Model(input_seq, Activation('softmax')(logits))\n",
        "\n",
        "  model.compile(loss=sparse_categorical_crossentropy,\n",
        "                optimizer=Adam(learning_rate),\n",
        "                metrics=['accuracy'])\n",
        "  \n",
        "  return model\n",
        "\n",
        "# tests.test_embed_model(embedding_model)\n",
        "\n",
        "#reshaping input\n",
        "tmp_x = pad(preprocessed_english_sentences, max_french_sequence_length)\n",
        "# tmp_x = tmp_x.reshape((-1, preprocessed_french_sentences.shape[-2], 1))\n",
        "\n",
        "#train the neural network\n",
        "embedding_rnn_model = embedding_model(\n",
        "    tmp_x.shape,\n",
        "    max_french_sequence_length,\n",
        "    english_vocab_size,\n",
        "    french_vocab_size)\n",
        "embedding_rnn_model.fit(tmp_x, preprocessed_french_sentences, batch_size=512, epochs=64, validation_split=0.2)\n",
        "\n",
        "print(logits_to_text(embedding_rnn_model.predict(tmp_x[:1])[0] , french_tokenizer))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 110288 samples, validate on 27573 samples\n",
            "Epoch 1/64\n",
            "110288/110288 [==============================] - 18s 167us/step - loss: 1.7954 - accuracy: 0.6056 - val_loss: nan - val_accuracy: 0.7819\n",
            "Epoch 2/64\n",
            "110288/110288 [==============================] - 16s 146us/step - loss: 0.6699 - accuracy: 0.8070 - val_loss: nan - val_accuracy: 0.8605\n",
            "Epoch 3/64\n",
            "110288/110288 [==============================] - 16s 144us/step - loss: 0.4885 - accuracy: 0.8545 - val_loss: nan - val_accuracy: 0.8880\n",
            "Epoch 4/64\n",
            "110288/110288 [==============================] - 15s 138us/step - loss: 0.4150 - accuracy: 0.8763 - val_loss: nan - val_accuracy: 0.9006\n",
            "Epoch 5/64\n",
            "110288/110288 [==============================] - 16s 144us/step - loss: 0.3732 - accuracy: 0.8881 - val_loss: nan - val_accuracy: 0.9051\n",
            "Epoch 6/64\n",
            "110288/110288 [==============================] - 16s 141us/step - loss: 0.3469 - accuracy: 0.8957 - val_loss: nan - val_accuracy: 0.9119\n",
            "Epoch 7/64\n",
            "110288/110288 [==============================] - 15s 137us/step - loss: 0.3287 - accuracy: 0.9006 - val_loss: nan - val_accuracy: 0.9153\n",
            "Epoch 8/64\n",
            "110288/110288 [==============================] - 16s 142us/step - loss: 0.3148 - accuracy: 0.9045 - val_loss: nan - val_accuracy: 0.9181\n",
            "Epoch 9/64\n",
            "110288/110288 [==============================] - 15s 139us/step - loss: 0.3055 - accuracy: 0.9069 - val_loss: nan - val_accuracy: 0.9200\n",
            "Epoch 10/64\n",
            "110288/110288 [==============================] - 15s 139us/step - loss: 0.2962 - accuracy: 0.9093 - val_loss: nan - val_accuracy: 0.9192\n",
            "Epoch 11/64\n",
            "110288/110288 [==============================] - 16s 142us/step - loss: 0.2905 - accuracy: 0.9111 - val_loss: nan - val_accuracy: 0.9214\n",
            "Epoch 12/64\n",
            "110288/110288 [==============================] - 16s 141us/step - loss: 0.2848 - accuracy: 0.9123 - val_loss: nan - val_accuracy: 0.9234\n",
            "Epoch 13/64\n",
            "110288/110288 [==============================] - 15s 138us/step - loss: 0.2794 - accuracy: 0.9139 - val_loss: nan - val_accuracy: 0.9223\n",
            "Epoch 14/64\n",
            "110288/110288 [==============================] - 15s 137us/step - loss: 0.2749 - accuracy: 0.9149 - val_loss: nan - val_accuracy: 0.9249\n",
            "Epoch 15/64\n",
            "110288/110288 [==============================] - 15s 136us/step - loss: 0.2752 - accuracy: 0.9148 - val_loss: nan - val_accuracy: 0.9200\n",
            "Epoch 16/64\n",
            "110288/110288 [==============================] - 15s 137us/step - loss: 0.2706 - accuracy: 0.9161 - val_loss: nan - val_accuracy: 0.9260\n",
            "Epoch 17/64\n",
            "110288/110288 [==============================] - 15s 139us/step - loss: 0.2705 - accuracy: 0.9161 - val_loss: nan - val_accuracy: 0.9251\n",
            "Epoch 18/64\n",
            "110288/110288 [==============================] - 15s 137us/step - loss: 0.2632 - accuracy: 0.9178 - val_loss: nan - val_accuracy: 0.9261\n",
            "Epoch 19/64\n",
            "110288/110288 [==============================] - 15s 136us/step - loss: 0.2621 - accuracy: 0.9181 - val_loss: nan - val_accuracy: 0.9252\n",
            "Epoch 20/64\n",
            "110288/110288 [==============================] - 15s 136us/step - loss: 0.2621 - accuracy: 0.9179 - val_loss: nan - val_accuracy: 0.9265\n",
            "Epoch 21/64\n",
            "110288/110288 [==============================] - 15s 135us/step - loss: 0.2624 - accuracy: 0.9178 - val_loss: nan - val_accuracy: 0.9264\n",
            "Epoch 22/64\n",
            "110288/110288 [==============================] - 15s 135us/step - loss: 0.2610 - accuracy: 0.9185 - val_loss: nan - val_accuracy: 0.9272\n",
            "Epoch 23/64\n",
            "110288/110288 [==============================] - 15s 139us/step - loss: 0.2579 - accuracy: 0.9191 - val_loss: nan - val_accuracy: 0.9282\n",
            "Epoch 24/64\n",
            "110288/110288 [==============================] - 15s 138us/step - loss: 0.2550 - accuracy: 0.9197 - val_loss: nan - val_accuracy: 0.9269\n",
            "Epoch 25/64\n",
            "110288/110288 [==============================] - 15s 136us/step - loss: 0.2580 - accuracy: 0.9191 - val_loss: nan - val_accuracy: 0.9270\n",
            "Epoch 26/64\n",
            "110288/110288 [==============================] - 15s 137us/step - loss: 0.2561 - accuracy: 0.9194 - val_loss: nan - val_accuracy: 0.9270\n",
            "Epoch 27/64\n",
            "110288/110288 [==============================] - 15s 136us/step - loss: 0.2538 - accuracy: 0.9201 - val_loss: nan - val_accuracy: 0.9266\n",
            "Epoch 28/64\n",
            "110288/110288 [==============================] - 15s 137us/step - loss: 0.2501 - accuracy: 0.9211 - val_loss: nan - val_accuracy: 0.9294\n",
            "Epoch 29/64\n",
            "110288/110288 [==============================] - 15s 138us/step - loss: 0.2563 - accuracy: 0.9195 - val_loss: nan - val_accuracy: 0.9273\n",
            "Epoch 30/64\n",
            "110288/110288 [==============================] - 15s 138us/step - loss: 0.2541 - accuracy: 0.9202 - val_loss: nan - val_accuracy: 0.9285\n",
            "Epoch 31/64\n",
            "110288/110288 [==============================] - 15s 136us/step - loss: 0.2508 - accuracy: 0.9208 - val_loss: nan - val_accuracy: 0.9279\n",
            "Epoch 32/64\n",
            "110288/110288 [==============================] - 15s 138us/step - loss: 0.2527 - accuracy: 0.9204 - val_loss: nan - val_accuracy: 0.9269\n",
            "Epoch 33/64\n",
            "110288/110288 [==============================] - 15s 136us/step - loss: 0.2512 - accuracy: 0.9208 - val_loss: nan - val_accuracy: 0.9241\n",
            "Epoch 34/64\n",
            "110288/110288 [==============================] - 15s 136us/step - loss: 0.2538 - accuracy: 0.9200 - val_loss: nan - val_accuracy: 0.9297\n",
            "Epoch 35/64\n",
            "110288/110288 [==============================] - 15s 137us/step - loss: 0.2492 - accuracy: 0.9213 - val_loss: nan - val_accuracy: 0.9293\n",
            "Epoch 36/64\n",
            "110288/110288 [==============================] - 15s 137us/step - loss: 0.2481 - accuracy: 0.9216 - val_loss: nan - val_accuracy: 0.9266\n",
            "Epoch 37/64\n",
            "110288/110288 [==============================] - 15s 136us/step - loss: 0.2514 - accuracy: 0.9207 - val_loss: nan - val_accuracy: 0.9290\n",
            "Epoch 38/64\n",
            "110288/110288 [==============================] - 15s 137us/step - loss: 0.2492 - accuracy: 0.9214 - val_loss: nan - val_accuracy: 0.9280\n",
            "Epoch 39/64\n",
            "110288/110288 [==============================] - 15s 136us/step - loss: 0.2458 - accuracy: 0.9222 - val_loss: nan - val_accuracy: 0.9307\n",
            "Epoch 40/64\n",
            "110288/110288 [==============================] - 15s 137us/step - loss: 0.2453 - accuracy: 0.9224 - val_loss: nan - val_accuracy: 0.9286\n",
            "Epoch 41/64\n",
            "110288/110288 [==============================] - 15s 139us/step - loss: 0.2474 - accuracy: 0.9217 - val_loss: nan - val_accuracy: 0.9264\n",
            "Epoch 42/64\n",
            "110288/110288 [==============================] - 15s 136us/step - loss: 0.2512 - accuracy: 0.9206 - val_loss: nan - val_accuracy: 0.9276\n",
            "Epoch 43/64\n",
            "110288/110288 [==============================] - 15s 137us/step - loss: 0.2549 - accuracy: 0.9195 - val_loss: nan - val_accuracy: 0.9300\n",
            "Epoch 44/64\n",
            "110288/110288 [==============================] - 16s 141us/step - loss: 0.2478 - accuracy: 0.9216 - val_loss: nan - val_accuracy: 0.9266\n",
            "Epoch 45/64\n",
            "110288/110288 [==============================] - 15s 136us/step - loss: 0.2535 - accuracy: 0.9199 - val_loss: nan - val_accuracy: 0.9284\n",
            "Epoch 46/64\n",
            "110288/110288 [==============================] - 15s 137us/step - loss: 0.2484 - accuracy: 0.9214 - val_loss: nan - val_accuracy: 0.9296\n",
            "Epoch 47/64\n",
            "110288/110288 [==============================] - 15s 137us/step - loss: 0.2470 - accuracy: 0.9218 - val_loss: nan - val_accuracy: 0.9284\n",
            "Epoch 48/64\n",
            "110288/110288 [==============================] - 15s 137us/step - loss: 0.2540 - accuracy: 0.9201 - val_loss: nan - val_accuracy: 0.9285\n",
            "Epoch 49/64\n",
            "110288/110288 [==============================] - 15s 137us/step - loss: 0.2478 - accuracy: 0.9216 - val_loss: nan - val_accuracy: 0.9280\n",
            "Epoch 50/64\n",
            "110288/110288 [==============================] - 15s 139us/step - loss: 0.2517 - accuracy: 0.9207 - val_loss: nan - val_accuracy: 0.9291\n",
            "Epoch 51/64\n",
            "110288/110288 [==============================] - 15s 138us/step - loss: 0.2517 - accuracy: 0.9207 - val_loss: nan - val_accuracy: 0.9286\n",
            "Epoch 52/64\n",
            "110288/110288 [==============================] - 15s 140us/step - loss: 0.2548 - accuracy: 0.9197 - val_loss: nan - val_accuracy: 0.9274\n",
            "Epoch 53/64\n",
            "110288/110288 [==============================] - 16s 141us/step - loss: 0.2539 - accuracy: 0.9202 - val_loss: nan - val_accuracy: 0.9285\n",
            "Epoch 54/64\n",
            "110288/110288 [==============================] - 16s 141us/step - loss: 0.2494 - accuracy: 0.9211 - val_loss: nan - val_accuracy: 0.9286\n",
            "Epoch 55/64\n",
            "110288/110288 [==============================] - 16s 142us/step - loss: 0.2538 - accuracy: 0.9201 - val_loss: nan - val_accuracy: 0.9285\n",
            "Epoch 56/64\n",
            "110288/110288 [==============================] - 16s 144us/step - loss: 0.2504 - accuracy: 0.9207 - val_loss: nan - val_accuracy: 0.9295\n",
            "Epoch 57/64\n",
            "110288/110288 [==============================] - 15s 140us/step - loss: 0.2500 - accuracy: 0.9213 - val_loss: nan - val_accuracy: 0.9279\n",
            "Epoch 58/64\n",
            "110288/110288 [==============================] - 15s 140us/step - loss: 0.2526 - accuracy: 0.9204 - val_loss: nan - val_accuracy: 0.9296\n",
            "Epoch 59/64\n",
            "110288/110288 [==============================] - 16s 142us/step - loss: 0.2455 - accuracy: 0.9224 - val_loss: nan - val_accuracy: 0.9295\n",
            "Epoch 60/64\n",
            "110288/110288 [==============================] - 16s 141us/step - loss: 0.2515 - accuracy: 0.9205 - val_loss: nan - val_accuracy: 0.9275\n",
            "Epoch 61/64\n",
            "110288/110288 [==============================] - 15s 140us/step - loss: 0.2496 - accuracy: 0.9214 - val_loss: nan - val_accuracy: 0.9299\n",
            "Epoch 62/64\n",
            "110288/110288 [==============================] - 15s 140us/step - loss: 0.2418 - accuracy: 0.9233 - val_loss: nan - val_accuracy: 0.9297\n",
            "Epoch 63/64\n",
            "110288/110288 [==============================] - 15s 139us/step - loss: 0.2581 - accuracy: 0.9190 - val_loss: nan - val_accuracy: 0.9273\n",
            "Epoch 64/64\n",
            "110288/110288 [==============================] - 16s 142us/step - loss: 0.2542 - accuracy: 0.9201 - val_loss: nan - val_accuracy: 0.9285\n",
            "new jersey est parfois calme en l'automne automne l' il est neigeux est en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iXsQn4mgP2u",
        "colab_type": "text"
      },
      "source": [
        "### Model 3: Bidirectional RNNs \n",
        "One restriction of a RNN is that it can't see the future input, only the past.  This is where bidirectional recurrent neural networks come in.  They are able to see the future data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEBd8saXXn2O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6ee3c760-b154-45a1-aed6-3272c7554ecb"
      },
      "source": [
        "def bidrectional_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "  \"\"\"\n",
        "  Build and train a bidirectional RNN model on x and y\n",
        "  :param input_shape: Tuple of input shape\n",
        "  :param output_sequence_length: Length of output sequence\n",
        "  :param english_vocab_size: Number of unique English words in the dataset\n",
        "  :param french_vocab_size: Number of unique French words in the dataset\n",
        "  :return: Keras model built, but not trained\n",
        "  \"\"\"\n",
        "\n",
        "  learning_rate = 1e-2\n",
        "\n",
        "  input_seq = Input(input_shape[1:])\n",
        "\n",
        "  rnn = Bidirectional(LSTM(64, return_sequences=True))(input_seq)\n",
        "\n",
        "  dropout = Dropout(0.2)(rnn)\n",
        "\n",
        "  logits = TimeDistributed(Dense(french_vocab_size))(dropout)\n",
        "\n",
        "  model = Model(input_seq, Activation('softmax')(logits))\n",
        "\n",
        "  model.compile(loss=sparse_categorical_crossentropy,\n",
        "                  optimizer=Adam(learning_rate),\n",
        "                  metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "# Reshaping the input to work with a basic RNN\n",
        "tmp_x = pad(preprocessed_english_sentences, max_french_sequence_length)\n",
        "tmp_x = tmp_x.reshape((-1, preprocessed_french_sentences.shape[-2], 1))\n",
        "\n",
        "# Train the neural network\n",
        "bidrectional_rnn_model = bidrectional_model(\n",
        "    tmp_x.shape,\n",
        "    max_french_sequence_length,\n",
        "    english_vocab_size,\n",
        "    french_vocab_size)\n",
        "bidrectional_rnn_model.fit(tmp_x, preprocessed_french_sentences, batch_size=512, epochs=64, validation_split=0.2)\n",
        "\n",
        "# Print prediction(s)\n",
        "print(logits_to_text(bidrectional_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 110288 samples, validate on 27573 samples\n",
            "Epoch 1/64\n",
            "110288/110288 [==============================] - 28s 253us/step - loss: 1.6933 - accuracy: 0.5827 - val_loss: nan - val_accuracy: 0.6481\n",
            "Epoch 2/64\n",
            "110288/110288 [==============================] - 27s 242us/step - loss: 1.2304 - accuracy: 0.6461 - val_loss: nan - val_accuracy: 0.6700\n",
            "Epoch 3/64\n",
            "110288/110288 [==============================] - 26s 238us/step - loss: 1.1412 - accuracy: 0.6619 - val_loss: nan - val_accuracy: 0.6851\n",
            "Epoch 4/64\n",
            "110288/110288 [==============================] - 26s 238us/step - loss: 1.0869 - accuracy: 0.6703 - val_loss: nan - val_accuracy: 0.6910\n",
            "Epoch 5/64\n",
            "110288/110288 [==============================] - 27s 241us/step - loss: 1.0497 - accuracy: 0.6761 - val_loss: nan - val_accuracy: 0.6962\n",
            "Epoch 6/64\n",
            "110288/110288 [==============================] - 26s 235us/step - loss: 1.0217 - accuracy: 0.6800 - val_loss: nan - val_accuracy: 0.6986\n",
            "Epoch 7/64\n",
            "110288/110288 [==============================] - 27s 241us/step - loss: 0.9997 - accuracy: 0.6838 - val_loss: nan - val_accuracy: 0.7011\n",
            "Epoch 8/64\n",
            "110288/110288 [==============================] - 26s 233us/step - loss: 0.9873 - accuracy: 0.6863 - val_loss: nan - val_accuracy: 0.7083\n",
            "Epoch 9/64\n",
            "110288/110288 [==============================] - 27s 242us/step - loss: 0.9695 - accuracy: 0.6899 - val_loss: nan - val_accuracy: 0.7059\n",
            "Epoch 10/64\n",
            "110288/110288 [==============================] - 25s 231us/step - loss: 0.9523 - accuracy: 0.6939 - val_loss: nan - val_accuracy: 0.7127\n",
            "Epoch 11/64\n",
            "110288/110288 [==============================] - 27s 242us/step - loss: 0.9379 - accuracy: 0.6986 - val_loss: nan - val_accuracy: 0.7252\n",
            "Epoch 12/64\n",
            "110288/110288 [==============================] - 26s 239us/step - loss: 0.9182 - accuracy: 0.7067 - val_loss: nan - val_accuracy: 0.7282\n",
            "Epoch 13/64\n",
            "110288/110288 [==============================] - 26s 236us/step - loss: 0.9235 - accuracy: 0.7021 - val_loss: nan - val_accuracy: 0.7248\n",
            "Epoch 14/64\n",
            "110288/110288 [==============================] - 27s 243us/step - loss: 0.8902 - accuracy: 0.7159 - val_loss: nan - val_accuracy: 0.7440\n",
            "Epoch 15/64\n",
            "110288/110288 [==============================] - 27s 242us/step - loss: 0.8874 - accuracy: 0.7193 - val_loss: nan - val_accuracy: 0.7493\n",
            "Epoch 16/64\n",
            "110288/110288 [==============================] - 27s 247us/step - loss: 0.8757 - accuracy: 0.7223 - val_loss: nan - val_accuracy: 0.7567\n",
            "Epoch 17/64\n",
            "110288/110288 [==============================] - 27s 241us/step - loss: 0.8645 - accuracy: 0.7247 - val_loss: nan - val_accuracy: 0.7575\n",
            "Epoch 18/64\n",
            "110288/110288 [==============================] - 27s 245us/step - loss: 0.8370 - accuracy: 0.7336 - val_loss: nan - val_accuracy: 0.7672\n",
            "Epoch 19/64\n",
            "110288/110288 [==============================] - 26s 238us/step - loss: 0.8170 - accuracy: 0.7399 - val_loss: nan - val_accuracy: 0.7642\n",
            "Epoch 20/64\n",
            "110288/110288 [==============================] - 27s 249us/step - loss: 0.8104 - accuracy: 0.7417 - val_loss: nan - val_accuracy: 0.7717\n",
            "Epoch 21/64\n",
            "110288/110288 [==============================] - 27s 240us/step - loss: 0.8189 - accuracy: 0.7390 - val_loss: nan - val_accuracy: 0.7535\n",
            "Epoch 22/64\n",
            "110288/110288 [==============================] - 26s 240us/step - loss: 0.7922 - accuracy: 0.7468 - val_loss: nan - val_accuracy: 0.7772\n",
            "Epoch 23/64\n",
            "110288/110288 [==============================] - 27s 242us/step - loss: 0.7856 - accuracy: 0.7491 - val_loss: nan - val_accuracy: 0.7811\n",
            "Epoch 24/64\n",
            "110288/110288 [==============================] - 26s 240us/step - loss: 0.8370 - accuracy: 0.7311 - val_loss: nan - val_accuracy: 0.7432\n",
            "Epoch 25/64\n",
            "110288/110288 [==============================] - 27s 244us/step - loss: 0.8472 - accuracy: 0.7247 - val_loss: nan - val_accuracy: 0.7670\n",
            "Epoch 26/64\n",
            "110288/110288 [==============================] - 27s 241us/step - loss: 0.7946 - accuracy: 0.7456 - val_loss: nan - val_accuracy: 0.7823\n",
            "Epoch 27/64\n",
            "110288/110288 [==============================] - 27s 248us/step - loss: 0.7824 - accuracy: 0.7502 - val_loss: nan - val_accuracy: 0.7851\n",
            "Epoch 28/64\n",
            "110288/110288 [==============================] - 26s 239us/step - loss: 0.7634 - accuracy: 0.7557 - val_loss: nan - val_accuracy: 0.7832\n",
            "Epoch 29/64\n",
            "110288/110288 [==============================] - 27s 240us/step - loss: 0.7693 - accuracy: 0.7541 - val_loss: nan - val_accuracy: 0.7883\n",
            "Epoch 30/64\n",
            "110288/110288 [==============================] - 27s 245us/step - loss: 0.7531 - accuracy: 0.7591 - val_loss: nan - val_accuracy: 0.7957\n",
            "Epoch 31/64\n",
            "110288/110288 [==============================] - 26s 240us/step - loss: 0.7420 - accuracy: 0.7627 - val_loss: nan - val_accuracy: 0.7930\n",
            "Epoch 32/64\n",
            "110288/110288 [==============================] - 27s 245us/step - loss: 0.7469 - accuracy: 0.7615 - val_loss: nan - val_accuracy: 0.7832\n",
            "Epoch 33/64\n",
            "110288/110288 [==============================] - 27s 241us/step - loss: 0.7395 - accuracy: 0.7642 - val_loss: nan - val_accuracy: 0.7952\n",
            "Epoch 34/64\n",
            "110288/110288 [==============================] - 27s 247us/step - loss: 0.7303 - accuracy: 0.7669 - val_loss: nan - val_accuracy: 0.7981\n",
            "Epoch 35/64\n",
            "110288/110288 [==============================] - 27s 242us/step - loss: 0.7254 - accuracy: 0.7686 - val_loss: nan - val_accuracy: 0.7979\n",
            "Epoch 36/64\n",
            "110288/110288 [==============================] - 28s 250us/step - loss: 0.7703 - accuracy: 0.7542 - val_loss: nan - val_accuracy: 0.7936\n",
            "Epoch 37/64\n",
            "110288/110288 [==============================] - 27s 241us/step - loss: 0.7280 - accuracy: 0.7676 - val_loss: nan - val_accuracy: 0.8000\n",
            "Epoch 38/64\n",
            "110288/110288 [==============================] - 27s 245us/step - loss: 0.7216 - accuracy: 0.7701 - val_loss: nan - val_accuracy: 0.7987\n",
            "Epoch 39/64\n",
            "110288/110288 [==============================] - 27s 246us/step - loss: 0.7582 - accuracy: 0.7582 - val_loss: nan - val_accuracy: 0.7544\n",
            "Epoch 40/64\n",
            "110288/110288 [==============================] - 27s 240us/step - loss: 0.7730 - accuracy: 0.7500 - val_loss: nan - val_accuracy: 0.7994\n",
            "Epoch 41/64\n",
            "110288/110288 [==============================] - 27s 246us/step - loss: 0.7252 - accuracy: 0.7682 - val_loss: nan - val_accuracy: 0.8082\n",
            "Epoch 42/64\n",
            "110288/110288 [==============================] - 26s 235us/step - loss: 0.7239 - accuracy: 0.7697 - val_loss: nan - val_accuracy: 0.7995\n",
            "Epoch 43/64\n",
            "110288/110288 [==============================] - 27s 244us/step - loss: 0.7115 - accuracy: 0.7735 - val_loss: nan - val_accuracy: 0.8116\n",
            "Epoch 44/64\n",
            "110288/110288 [==============================] - 26s 238us/step - loss: 0.7037 - accuracy: 0.7760 - val_loss: nan - val_accuracy: 0.8122\n",
            "Epoch 45/64\n",
            "110288/110288 [==============================] - 26s 234us/step - loss: 0.7002 - accuracy: 0.7770 - val_loss: nan - val_accuracy: 0.8097\n",
            "Epoch 46/64\n",
            "110288/110288 [==============================] - 26s 240us/step - loss: 0.6975 - accuracy: 0.7780 - val_loss: nan - val_accuracy: 0.8087\n",
            "Epoch 47/64\n",
            "110288/110288 [==============================] - 26s 234us/step - loss: 0.7026 - accuracy: 0.7765 - val_loss: nan - val_accuracy: 0.8129\n",
            "Epoch 48/64\n",
            "110288/110288 [==============================] - 26s 237us/step - loss: 0.7354 - accuracy: 0.7666 - val_loss: nan - val_accuracy: 0.8142\n",
            "Epoch 49/64\n",
            "110288/110288 [==============================] - 26s 236us/step - loss: 0.6950 - accuracy: 0.7795 - val_loss: nan - val_accuracy: 0.8136\n",
            "Epoch 50/64\n",
            "110288/110288 [==============================] - 27s 244us/step - loss: 0.6917 - accuracy: 0.7802 - val_loss: nan - val_accuracy: 0.8003\n",
            "Epoch 51/64\n",
            "110288/110288 [==============================] - 26s 236us/step - loss: 0.7000 - accuracy: 0.7780 - val_loss: nan - val_accuracy: 0.8199\n",
            "Epoch 52/64\n",
            "110288/110288 [==============================] - 27s 242us/step - loss: 0.7139 - accuracy: 0.7736 - val_loss: nan - val_accuracy: 0.8129\n",
            "Epoch 53/64\n",
            "110288/110288 [==============================] - 26s 235us/step - loss: 0.6857 - accuracy: 0.7832 - val_loss: nan - val_accuracy: 0.8196\n",
            "Epoch 54/64\n",
            "110288/110288 [==============================] - 26s 240us/step - loss: 0.6778 - accuracy: 0.7854 - val_loss: nan - val_accuracy: 0.8200\n",
            "Epoch 55/64\n",
            "110288/110288 [==============================] - 26s 236us/step - loss: 0.6748 - accuracy: 0.7856 - val_loss: nan - val_accuracy: 0.8216\n",
            "Epoch 56/64\n",
            "110288/110288 [==============================] - 26s 236us/step - loss: 0.6734 - accuracy: 0.7864 - val_loss: nan - val_accuracy: 0.8268\n",
            "Epoch 57/64\n",
            "110288/110288 [==============================] - 26s 237us/step - loss: 0.6684 - accuracy: 0.7878 - val_loss: nan - val_accuracy: 0.8235\n",
            "Epoch 58/64\n",
            "110288/110288 [==============================] - 26s 235us/step - loss: 0.6829 - accuracy: 0.7840 - val_loss: nan - val_accuracy: 0.8121\n",
            "Epoch 59/64\n",
            "110288/110288 [==============================] - 26s 240us/step - loss: 0.6706 - accuracy: 0.7873 - val_loss: nan - val_accuracy: 0.8197\n",
            "Epoch 60/64\n",
            "110288/110288 [==============================] - 26s 234us/step - loss: 0.6633 - accuracy: 0.7899 - val_loss: nan - val_accuracy: 0.8249\n",
            "Epoch 61/64\n",
            "110288/110288 [==============================] - 26s 240us/step - loss: 0.6978 - accuracy: 0.7793 - val_loss: nan - val_accuracy: 0.8180\n",
            "Epoch 62/64\n",
            "110288/110288 [==============================] - 27s 242us/step - loss: 0.6884 - accuracy: 0.7822 - val_loss: nan - val_accuracy: 0.7833\n",
            "Epoch 63/64\n",
            "110288/110288 [==============================] - 27s 246us/step - loss: 0.6625 - accuracy: 0.7898 - val_loss: nan - val_accuracy: 0.8273\n",
            "Epoch 64/64\n",
            "110288/110288 [==============================] - 26s 237us/step - loss: 0.6606 - accuracy: 0.7908 - val_loss: nan - val_accuracy: 0.8272\n",
            "new jersey est parfois calme en mois de l' il est il en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvsN9XhZexLI",
        "colab_type": "text"
      },
      "source": [
        "### Model 4: Encoder-Decoder\n",
        "This model is made up of an encoder and decoder. The encoder creates a matrix representation of the sentence. The decoder takes this matrix as input and predicts the translation as output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVPWFyKhavYg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f6d47025-b433-43c5-fe4c-950290996db9"
      },
      "source": [
        "from keras.layers import RepeatVector\n",
        "\n",
        "def encdec_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "    \"\"\"\n",
        "    Build and train an encoder-decoder model on x and y\n",
        "    :param input_shape: Tuple of input shape\n",
        "    :param output_sequence_length: Length of output sequence\n",
        "    :param english_vocab_size: Number of unique English words in the dataset\n",
        "    :param french_vocab_size: Number of unique French words in the dataset\n",
        "    :return: Keras model built, but not trained\n",
        "    \"\"\"\n",
        "    \n",
        "\n",
        "    learning_rate = 0.03\n",
        "\n",
        "    #Encoder\n",
        "    encoder_inputs = Input(shape=input_shape[1:])\n",
        "    encoder_gru = GRU(output_sequence_length)(encoder_inputs)\n",
        "    encoder_outputs = Dense(64, activation='relu')(encoder_gru)\n",
        "    \n",
        "    #Decoder\n",
        "    decoder_inputs = RepeatVector(output_sequence_length)(encoder_outputs)\n",
        "    decoder_gru = GRU(64, return_sequences=True)(decoder_inputs)\n",
        "    output_layer = TimeDistributed(Dense(french_vocab_size, activation='softmax'))\n",
        "    outputs = output_layer(decoder_gru)\n",
        "\n",
        "    #Create Model from parameters defined above\n",
        "    model = Model(inputs=encoder_inputs, outputs=outputs)\n",
        "    model.compile(loss=sparse_categorical_crossentropy,\n",
        "                  optimizer=Adam(learning_rate),\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "#reshape input\n",
        "tmp_x = pad(preprocessed_english_sentences, max_french_sequence_length)\n",
        "tmp_x = tmp_x.reshape((-1, preprocessed_french_sentences.shape[-2], 1))\n",
        "\n",
        "# Train the neural network\n",
        "encdec_rnn_model = encdec_model(\n",
        "    tmp_x.shape,\n",
        "    max_french_sequence_length,\n",
        "    english_vocab_size,\n",
        "    french_vocab_size)\n",
        "encdec_rnn_model.fit(tmp_x, preprocessed_french_sentences, batch_size=512, epochs=64, validation_split=0.2)\n",
        "\n",
        "# Print prediction(s)\n",
        "print(logits_to_text(encdec_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 110288 samples, validate on 27573 samples\n",
            "Epoch 1/64\n",
            "110288/110288 [==============================] - 23s 205us/step - loss: 2.3537 - accuracy: 0.4879 - val_loss: nan - val_accuracy: 0.5356\n",
            "Epoch 2/64\n",
            "110288/110288 [==============================] - 21s 186us/step - loss: 1.6896 - accuracy: 0.5542 - val_loss: nan - val_accuracy: 0.5664\n",
            "Epoch 3/64\n",
            "110288/110288 [==============================] - 20s 185us/step - loss: 1.5182 - accuracy: 0.5870 - val_loss: nan - val_accuracy: 0.6119\n",
            "Epoch 4/64\n",
            "110288/110288 [==============================] - 21s 194us/step - loss: 1.4224 - accuracy: 0.6068 - val_loss: nan - val_accuracy: 0.6144\n",
            "Epoch 5/64\n",
            "110288/110288 [==============================] - 21s 188us/step - loss: 1.3816 - accuracy: 0.6129 - val_loss: nan - val_accuracy: 0.6064\n",
            "Epoch 6/64\n",
            "110288/110288 [==============================] - 21s 188us/step - loss: 1.3888 - accuracy: 0.6096 - val_loss: nan - val_accuracy: 0.6105\n",
            "Epoch 7/64\n",
            "110288/110288 [==============================] - 20s 179us/step - loss: 1.3482 - accuracy: 0.6176 - val_loss: nan - val_accuracy: 0.6289\n",
            "Epoch 8/64\n",
            "110288/110288 [==============================] - 21s 186us/step - loss: 1.3083 - accuracy: 0.6273 - val_loss: nan - val_accuracy: 0.6336\n",
            "Epoch 9/64\n",
            "110288/110288 [==============================] - 21s 187us/step - loss: 1.2831 - accuracy: 0.6339 - val_loss: nan - val_accuracy: 0.6381\n",
            "Epoch 10/64\n",
            "110288/110288 [==============================] - 20s 186us/step - loss: 1.4095 - accuracy: 0.6073 - val_loss: nan - val_accuracy: 0.6340\n",
            "Epoch 11/64\n",
            "110288/110288 [==============================] - 21s 191us/step - loss: 1.2852 - accuracy: 0.6329 - val_loss: nan - val_accuracy: 0.6350\n",
            "Epoch 12/64\n",
            "110288/110288 [==============================] - 20s 186us/step - loss: 1.2685 - accuracy: 0.6365 - val_loss: nan - val_accuracy: 0.6420\n",
            "Epoch 13/64\n",
            "110288/110288 [==============================] - 20s 182us/step - loss: 1.2940 - accuracy: 0.6307 - val_loss: nan - val_accuracy: 0.6161\n",
            "Epoch 14/64\n",
            "110288/110288 [==============================] - 20s 182us/step - loss: 1.2845 - accuracy: 0.6333 - val_loss: nan - val_accuracy: 0.6410\n",
            "Epoch 15/64\n",
            "110288/110288 [==============================] - 20s 184us/step - loss: 1.3412 - accuracy: 0.6259 - val_loss: nan - val_accuracy: 0.6443\n",
            "Epoch 16/64\n",
            "110288/110288 [==============================] - 20s 185us/step - loss: 1.2516 - accuracy: 0.6420 - val_loss: nan - val_accuracy: 0.6454\n",
            "Epoch 17/64\n",
            "110288/110288 [==============================] - 21s 195us/step - loss: 1.3127 - accuracy: 0.6302 - val_loss: nan - val_accuracy: 0.6231\n",
            "Epoch 18/64\n",
            "110288/110288 [==============================] - 21s 188us/step - loss: 1.2939 - accuracy: 0.6357 - val_loss: nan - val_accuracy: 0.6497\n",
            "Epoch 19/64\n",
            "110288/110288 [==============================] - 21s 187us/step - loss: 1.2516 - accuracy: 0.6445 - val_loss: nan - val_accuracy: 0.6478\n",
            "Epoch 20/64\n",
            "110288/110288 [==============================] - 21s 186us/step - loss: 1.3225 - accuracy: 0.6294 - val_loss: nan - val_accuracy: 0.6521\n",
            "Epoch 21/64\n",
            "110288/110288 [==============================] - 21s 186us/step - loss: 1.3254 - accuracy: 0.6315 - val_loss: nan - val_accuracy: 0.6403\n",
            "Epoch 22/64\n",
            "110288/110288 [==============================] - 20s 185us/step - loss: 1.2739 - accuracy: 0.6406 - val_loss: nan - val_accuracy: 0.6374\n",
            "Epoch 23/64\n",
            "110288/110288 [==============================] - 20s 185us/step - loss: 1.2641 - accuracy: 0.6414 - val_loss: nan - val_accuracy: 0.6499\n",
            "Epoch 24/64\n",
            "110288/110288 [==============================] - 21s 191us/step - loss: 1.2828 - accuracy: 0.6379 - val_loss: nan - val_accuracy: 0.6466\n",
            "Epoch 25/64\n",
            "110288/110288 [==============================] - 21s 190us/step - loss: 1.2473 - accuracy: 0.6436 - val_loss: nan - val_accuracy: 0.6467\n",
            "Epoch 26/64\n",
            "110288/110288 [==============================] - 20s 184us/step - loss: 1.2533 - accuracy: 0.6421 - val_loss: nan - val_accuracy: 0.6451\n",
            "Epoch 27/64\n",
            "110288/110288 [==============================] - 20s 184us/step - loss: 1.2861 - accuracy: 0.6374 - val_loss: nan - val_accuracy: 0.6530\n",
            "Epoch 28/64\n",
            "110288/110288 [==============================] - 21s 186us/step - loss: 1.2556 - accuracy: 0.6452 - val_loss: nan - val_accuracy: 0.6610\n",
            "Epoch 29/64\n",
            "110288/110288 [==============================] - 21s 191us/step - loss: 1.4100 - accuracy: 0.6149 - val_loss: nan - val_accuracy: 0.6252\n",
            "Epoch 30/64\n",
            "110288/110288 [==============================] - 20s 185us/step - loss: 1.2870 - accuracy: 0.6378 - val_loss: nan - val_accuracy: 0.6528\n",
            "Epoch 31/64\n",
            "110288/110288 [==============================] - 21s 186us/step - loss: 1.3958 - accuracy: 0.6173 - val_loss: nan - val_accuracy: 0.6247\n",
            "Epoch 32/64\n",
            "110288/110288 [==============================] - 20s 184us/step - loss: 1.3299 - accuracy: 0.6281 - val_loss: nan - val_accuracy: 0.6388\n",
            "Epoch 33/64\n",
            "110288/110288 [==============================] - 20s 184us/step - loss: 1.2558 - accuracy: 0.6439 - val_loss: nan - val_accuracy: 0.6508\n",
            "Epoch 34/64\n",
            "110288/110288 [==============================] - 20s 181us/step - loss: 1.2865 - accuracy: 0.6382 - val_loss: nan - val_accuracy: 0.6158\n",
            "Epoch 35/64\n",
            "110288/110288 [==============================] - 20s 186us/step - loss: 1.2576 - accuracy: 0.6444 - val_loss: nan - val_accuracy: 0.6551\n",
            "Epoch 36/64\n",
            "110288/110288 [==============================] - 20s 177us/step - loss: 1.3712 - accuracy: 0.6232 - val_loss: nan - val_accuracy: 0.6077\n",
            "Epoch 37/64\n",
            "110288/110288 [==============================] - 21s 192us/step - loss: 1.3104 - accuracy: 0.6305 - val_loss: nan - val_accuracy: 0.6484\n",
            "Epoch 38/64\n",
            "110288/110288 [==============================] - 21s 191us/step - loss: 1.2967 - accuracy: 0.6379 - val_loss: nan - val_accuracy: 0.6347\n",
            "Epoch 39/64\n",
            "110288/110288 [==============================] - 22s 198us/step - loss: 1.2783 - accuracy: 0.6417 - val_loss: nan - val_accuracy: 0.6507\n",
            "Epoch 40/64\n",
            "110288/110288 [==============================] - 21s 190us/step - loss: 1.2915 - accuracy: 0.6404 - val_loss: nan - val_accuracy: 0.6187\n",
            "Epoch 41/64\n",
            "110288/110288 [==============================] - 21s 187us/step - loss: 1.5886 - accuracy: 0.5994 - val_loss: nan - val_accuracy: 0.5895\n",
            "Epoch 42/64\n",
            "110288/110288 [==============================] - 21s 194us/step - loss: 1.4931 - accuracy: 0.6110 - val_loss: nan - val_accuracy: 0.6083\n",
            "Epoch 43/64\n",
            "110288/110288 [==============================] - 21s 186us/step - loss: 1.4003 - accuracy: 0.6260 - val_loss: nan - val_accuracy: 0.6408\n",
            "Epoch 44/64\n",
            "110288/110288 [==============================] - 21s 188us/step - loss: 1.3875 - accuracy: 0.6263 - val_loss: nan - val_accuracy: 0.6249\n",
            "Epoch 45/64\n",
            "110288/110288 [==============================] - 21s 187us/step - loss: 1.3708 - accuracy: 0.6294 - val_loss: nan - val_accuracy: 0.6299\n",
            "Epoch 46/64\n",
            "110288/110288 [==============================] - 21s 188us/step - loss: 1.3586 - accuracy: 0.6307 - val_loss: nan - val_accuracy: 0.6218\n",
            "Epoch 47/64\n",
            "110288/110288 [==============================] - 21s 193us/step - loss: 1.3224 - accuracy: 0.6388 - val_loss: nan - val_accuracy: 0.6498\n",
            "Epoch 48/64\n",
            "110288/110288 [==============================] - 21s 186us/step - loss: 1.3659 - accuracy: 0.6332 - val_loss: nan - val_accuracy: 0.6105\n",
            "Epoch 49/64\n",
            "110288/110288 [==============================] - 21s 190us/step - loss: 1.3517 - accuracy: 0.6326 - val_loss: nan - val_accuracy: 0.5983\n",
            "Epoch 50/64\n",
            "110288/110288 [==============================] - 21s 187us/step - loss: 1.5036 - accuracy: 0.6048 - val_loss: nan - val_accuracy: 0.6218\n",
            "Epoch 51/64\n",
            "110288/110288 [==============================] - 20s 185us/step - loss: 1.4104 - accuracy: 0.6170 - val_loss: nan - val_accuracy: 0.6275\n",
            "Epoch 52/64\n",
            "110288/110288 [==============================] - 20s 183us/step - loss: 1.4084 - accuracy: 0.6185 - val_loss: nan - val_accuracy: 0.6216\n",
            "Epoch 53/64\n",
            "110288/110288 [==============================] - 21s 187us/step - loss: 1.4096 - accuracy: 0.6180 - val_loss: nan - val_accuracy: 0.6123\n",
            "Epoch 54/64\n",
            "110288/110288 [==============================] - 21s 188us/step - loss: 1.3880 - accuracy: 0.6181 - val_loss: nan - val_accuracy: 0.6024\n",
            "Epoch 55/64\n",
            "110288/110288 [==============================] - 21s 189us/step - loss: 1.3402 - accuracy: 0.6334 - val_loss: nan - val_accuracy: 0.6042\n",
            "Epoch 56/64\n",
            "110288/110288 [==============================] - 20s 184us/step - loss: 1.3235 - accuracy: 0.6385 - val_loss: nan - val_accuracy: 0.6359\n",
            "Epoch 57/64\n",
            "110288/110288 [==============================] - 21s 188us/step - loss: 1.3709 - accuracy: 0.6289 - val_loss: nan - val_accuracy: 0.6422\n",
            "Epoch 58/64\n",
            "110288/110288 [==============================] - 21s 191us/step - loss: 1.3272 - accuracy: 0.6359 - val_loss: nan - val_accuracy: 0.6395\n",
            "Epoch 59/64\n",
            "110288/110288 [==============================] - 20s 183us/step - loss: 1.3930 - accuracy: 0.6237 - val_loss: nan - val_accuracy: 0.5869\n",
            "Epoch 60/64\n",
            "110288/110288 [==============================] - 21s 186us/step - loss: 1.3838 - accuracy: 0.6293 - val_loss: nan - val_accuracy: 0.6449\n",
            "Epoch 61/64\n",
            "110288/110288 [==============================] - 20s 182us/step - loss: 1.3446 - accuracy: 0.6374 - val_loss: nan - val_accuracy: 0.6127\n",
            "Epoch 62/64\n",
            "110288/110288 [==============================] - 20s 183us/step - loss: 1.4839 - accuracy: 0.6112 - val_loss: nan - val_accuracy: 0.6131\n",
            "Epoch 63/64\n",
            "110288/110288 [==============================] - 20s 182us/step - loss: 1.3573 - accuracy: 0.6325 - val_loss: nan - val_accuracy: 0.6499\n",
            "Epoch 64/64\n",
            "110288/110288 [==============================] - 20s 179us/step - loss: 1.3590 - accuracy: 0.6338 - val_loss: nan - val_accuracy: 0.6531\n",
            "new jersey est jamais agréable en l' et il est il est en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GA0iblOJfhLX",
        "colab_type": "text"
      },
      "source": [
        "### Model 5: Custom \n",
        "Use everything from the previous models to create a model that incorporates embedding and a bidirectional rnn into one model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHd6CsR-YarW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2a39fbe2-1ed4-4088-9c1b-f54d7a632bc3"
      },
      "source": [
        "def model_final_GRU(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "    \"\"\"\n",
        "    Build and train a model that incorporates embedding, encoder-decoder, and bidirectional RNN on x and y\n",
        "    :param input_shape: Tuple of input shape\n",
        "    :param output_sequence_length: Length of output sequence\n",
        "    :param english_vocab_size: Number of unique English words in the dataset\n",
        "    :param french_vocab_size: Number of unique French words in the dataset\n",
        "    :return: Keras model built, but not trained\n",
        "    \"\"\"\n",
        "    # TODO: Implement\n",
        "    learning_rate = 1e-2\n",
        "    \n",
        "    input_seq = Input(input_shape[1:])\n",
        "\n",
        "    embed_layer = Embedding(english_vocab_size , 64 , input_length = output_sequence_length)(input_seq)\n",
        "    \n",
        "    rnn = Bidirectional(GRU(64, return_sequences=True))(embed_layer)\n",
        "    \n",
        "    dropout = Dropout(0.2)(rnn)\n",
        "    \n",
        "    logits = TimeDistributed(Dense(french_vocab_size))(dropout)\n",
        "    \n",
        " \n",
        "    model = Model(input_seq, Activation('softmax')(logits))\n",
        "    model.compile(loss=sparse_categorical_crossentropy,\n",
        "                  optimizer=Adam(learning_rate),\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    \n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "print('Final Model Loaded')\n",
        "# TODO: Train the final model\n",
        "\n",
        "tmp_x = pad(preprocessed_english_sentences, max_french_sequence_length)\n",
        "#tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
        "\n",
        "# Train the neural network\n",
        "final_gru_rnn_model = model_final_GRU(\n",
        "    tmp_x.shape,\n",
        "    max_french_sequence_length,\n",
        "    english_vocab_size,\n",
        "    french_vocab_size)\n",
        "final_gru_rnn_model.fit(tmp_x, preprocessed_french_sentences, batch_size=512, epochs=48, validation_split=0.2)\n",
        "\n",
        "# Print prediction(s)\n",
        "print(logits_to_text(final_gru_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final Model Loaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 110288 samples, validate on 27573 samples\n",
            "Epoch 1/48\n",
            "110288/110288 [==============================] - 23s 210us/step - loss: 1.1498 - accuracy: 0.7211 - val_loss: nan - val_accuracy: 0.8767\n",
            "Epoch 2/48\n",
            "110288/110288 [==============================] - 22s 202us/step - loss: 0.3666 - accuracy: 0.8889 - val_loss: nan - val_accuracy: 0.9279\n",
            "Epoch 3/48\n",
            "110288/110288 [==============================] - 23s 208us/step - loss: 0.2712 - accuracy: 0.9201 - val_loss: nan - val_accuracy: 0.9440\n",
            "Epoch 4/48\n",
            "110288/110288 [==============================] - 22s 202us/step - loss: 0.2335 - accuracy: 0.9325 - val_loss: nan - val_accuracy: 0.9500\n",
            "Epoch 5/48\n",
            "110288/110288 [==============================] - 22s 201us/step - loss: 0.2079 - accuracy: 0.9403 - val_loss: nan - val_accuracy: 0.9539\n",
            "Epoch 6/48\n",
            "110288/110288 [==============================] - 23s 213us/step - loss: 0.1963 - accuracy: 0.9440 - val_loss: nan - val_accuracy: 0.9574\n",
            "Epoch 7/48\n",
            "110288/110288 [==============================] - 23s 204us/step - loss: 0.1862 - accuracy: 0.9471 - val_loss: nan - val_accuracy: 0.9588\n",
            "Epoch 8/48\n",
            "110288/110288 [==============================] - 22s 202us/step - loss: 0.1841 - accuracy: 0.9477 - val_loss: nan - val_accuracy: 0.9607\n",
            "Epoch 9/48\n",
            "110288/110288 [==============================] - 22s 202us/step - loss: 0.1799 - accuracy: 0.9491 - val_loss: nan - val_accuracy: 0.9578\n",
            "Epoch 10/48\n",
            "110288/110288 [==============================] - 22s 200us/step - loss: 0.1730 - accuracy: 0.9510 - val_loss: nan - val_accuracy: 0.9613\n",
            "Epoch 11/48\n",
            "110288/110288 [==============================] - 22s 202us/step - loss: 0.1678 - accuracy: 0.9528 - val_loss: nan - val_accuracy: 0.9611\n",
            "Epoch 12/48\n",
            "110288/110288 [==============================] - 22s 202us/step - loss: 0.1849 - accuracy: 0.9479 - val_loss: nan - val_accuracy: 0.9598\n",
            "Epoch 13/48\n",
            "110288/110288 [==============================] - 22s 199us/step - loss: 0.1764 - accuracy: 0.9504 - val_loss: nan - val_accuracy: 0.9624\n",
            "Epoch 14/48\n",
            "110288/110288 [==============================] - 22s 203us/step - loss: 0.1778 - accuracy: 0.9502 - val_loss: nan - val_accuracy: 0.9620\n",
            "Epoch 15/48\n",
            "110288/110288 [==============================] - 22s 201us/step - loss: 0.1813 - accuracy: 0.9492 - val_loss: nan - val_accuracy: 0.9585\n",
            "Epoch 16/48\n",
            "110288/110288 [==============================] - 22s 197us/step - loss: 0.1790 - accuracy: 0.9495 - val_loss: nan - val_accuracy: 0.9629\n",
            "Epoch 17/48\n",
            "110288/110288 [==============================] - 22s 201us/step - loss: 0.1720 - accuracy: 0.9517 - val_loss: nan - val_accuracy: 0.9610\n",
            "Epoch 18/48\n",
            "110288/110288 [==============================] - 22s 200us/step - loss: 0.1679 - accuracy: 0.9529 - val_loss: nan - val_accuracy: 0.9624\n",
            "Epoch 19/48\n",
            "110288/110288 [==============================] - 23s 204us/step - loss: 0.1691 - accuracy: 0.9529 - val_loss: nan - val_accuracy: 0.9631\n",
            "Epoch 20/48\n",
            "110288/110288 [==============================] - 23s 205us/step - loss: 0.1656 - accuracy: 0.9539 - val_loss: nan - val_accuracy: 0.9641\n",
            "Epoch 21/48\n",
            "110288/110288 [==============================] - 22s 198us/step - loss: 0.1770 - accuracy: 0.9506 - val_loss: nan - val_accuracy: 0.9607\n",
            "Epoch 22/48\n",
            "110288/110288 [==============================] - 22s 203us/step - loss: 0.1760 - accuracy: 0.9507 - val_loss: nan - val_accuracy: 0.9632\n",
            "Epoch 23/48\n",
            "110288/110288 [==============================] - 22s 203us/step - loss: 0.1687 - accuracy: 0.9531 - val_loss: nan - val_accuracy: 0.9626\n",
            "Epoch 24/48\n",
            "110288/110288 [==============================] - 22s 197us/step - loss: 0.1718 - accuracy: 0.9524 - val_loss: nan - val_accuracy: 0.9584\n",
            "Epoch 25/48\n",
            "110288/110288 [==============================] - 22s 200us/step - loss: 0.1810 - accuracy: 0.9503 - val_loss: nan - val_accuracy: 0.9537\n",
            "Epoch 26/48\n",
            "110288/110288 [==============================] - 22s 202us/step - loss: 0.1863 - accuracy: 0.9478 - val_loss: nan - val_accuracy: 0.9611\n",
            "Epoch 27/48\n",
            "110288/110288 [==============================] - 23s 204us/step - loss: 0.1781 - accuracy: 0.9501 - val_loss: nan - val_accuracy: 0.9600\n",
            "Epoch 28/48\n",
            "110288/110288 [==============================] - 23s 205us/step - loss: 0.1811 - accuracy: 0.9492 - val_loss: nan - val_accuracy: 0.9559\n",
            "Epoch 29/48\n",
            "110288/110288 [==============================] - 22s 201us/step - loss: 0.1875 - accuracy: 0.9480 - val_loss: nan - val_accuracy: 0.9627\n",
            "Epoch 30/48\n",
            "110288/110288 [==============================] - 22s 200us/step - loss: 0.1762 - accuracy: 0.9512 - val_loss: nan - val_accuracy: 0.9596\n",
            "Epoch 31/48\n",
            "110288/110288 [==============================] - 22s 199us/step - loss: 0.1787 - accuracy: 0.9504 - val_loss: nan - val_accuracy: 0.9616\n",
            "Epoch 32/48\n",
            "110288/110288 [==============================] - 22s 201us/step - loss: 0.1765 - accuracy: 0.9512 - val_loss: nan - val_accuracy: 0.9608\n",
            "Epoch 33/48\n",
            "110288/110288 [==============================] - 22s 203us/step - loss: 0.1996 - accuracy: 0.9446 - val_loss: nan - val_accuracy: 0.9542\n",
            "Epoch 34/48\n",
            "110288/110288 [==============================] - 22s 204us/step - loss: 0.2028 - accuracy: 0.9437 - val_loss: nan - val_accuracy: 0.9567\n",
            "Epoch 35/48\n",
            "110288/110288 [==============================] - 23s 206us/step - loss: 0.1926 - accuracy: 0.9464 - val_loss: nan - val_accuracy: 0.9553\n",
            "Epoch 36/48\n",
            "110288/110288 [==============================] - 23s 204us/step - loss: 0.1964 - accuracy: 0.9457 - val_loss: nan - val_accuracy: 0.9575\n",
            "Epoch 37/48\n",
            "110288/110288 [==============================] - 22s 203us/step - loss: 0.1871 - accuracy: 0.9484 - val_loss: nan - val_accuracy: 0.9587\n",
            "Epoch 38/48\n",
            "110288/110288 [==============================] - 22s 200us/step - loss: 0.2034 - accuracy: 0.9441 - val_loss: nan - val_accuracy: 0.9573\n",
            "Epoch 39/48\n",
            "110288/110288 [==============================] - 23s 208us/step - loss: 0.1884 - accuracy: 0.9481 - val_loss: nan - val_accuracy: 0.9566\n",
            "Epoch 40/48\n",
            "110288/110288 [==============================] - 22s 200us/step - loss: 0.1986 - accuracy: 0.9450 - val_loss: nan - val_accuracy: 0.9519\n",
            "Epoch 41/48\n",
            "110288/110288 [==============================] - 22s 200us/step - loss: 0.2139 - accuracy: 0.9404 - val_loss: nan - val_accuracy: 0.9466\n",
            "Epoch 42/48\n",
            "110288/110288 [==============================] - 23s 210us/step - loss: 0.2523 - accuracy: 0.9284 - val_loss: nan - val_accuracy: 0.9477\n",
            "Epoch 43/48\n",
            "110288/110288 [==============================] - 23s 206us/step - loss: 0.2203 - accuracy: 0.9386 - val_loss: nan - val_accuracy: 0.9513\n",
            "Epoch 44/48\n",
            "110288/110288 [==============================] - 23s 204us/step - loss: 0.2217 - accuracy: 0.9383 - val_loss: nan - val_accuracy: 0.9504\n",
            "Epoch 45/48\n",
            "110288/110288 [==============================] - 22s 198us/step - loss: 0.2193 - accuracy: 0.9387 - val_loss: nan - val_accuracy: 0.9528\n",
            "Epoch 46/48\n",
            "110288/110288 [==============================] - 22s 202us/step - loss: 0.2296 - accuracy: 0.9360 - val_loss: nan - val_accuracy: 0.9457\n",
            "Epoch 47/48\n",
            "110288/110288 [==============================] - 23s 213us/step - loss: 0.2322 - accuracy: 0.9349 - val_loss: nan - val_accuracy: 0.9501\n",
            "Epoch 48/48\n",
            "110288/110288 [==============================] - 23s 212us/step - loss: 0.2151 - accuracy: 0.9401 - val_loss: nan - val_accuracy: 0.9520\n",
            "new jersey est parfois calme au l' automne il il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ljv-Fz_qgAyX",
        "colab_type": "text"
      },
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3p9xwybtf3Pl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "90b1b126-85a2-4322-88dd-a8f0353c7bf3"
      },
      "source": [
        "def final_predictions_GRU(x, y, x_tk, y_tk):\n",
        "    \"\"\"\n",
        "    Gets predictions using the final model\n",
        "    :param x: Preprocessed English data\n",
        "    :param y: Preprocessed French data\n",
        "    :param x_tk: English tokenizer\n",
        "    :param y_tk: French tokenizer\n",
        "    \"\"\"\n",
        "    # TODO: Train neural network using model_final \n",
        "    max_french_sequence_length = y.shape[1]\n",
        "    english_vocab_size = len(x_tk.word_index)\n",
        "    french_vocab_size = len(y_tk.word_index)\n",
        "\n",
        "    # Pad the input\n",
        "    x = pad(x, max_french_sequence_length)\n",
        "\n",
        "    # Train\n",
        "    model = model_final_GRU(\n",
        "        x.shape,\n",
        "        max_french_sequence_length,\n",
        "        english_vocab_size,\n",
        "        french_vocab_size)\n",
        "\n",
        "    model.fit(x, y, batch_size=512, epochs=80, validation_split=0.2)\n",
        "    \n",
        "    \n",
        "    ## DON'T EDIT ANYTHING BELOW THIS LINE\n",
        "    y_id_to_word = {value: key for key, value in y_tk.word_index.items()}\n",
        "    y_id_to_word[0] = '<PAD>'\n",
        "\n",
        "    sentence = 'he saw a old yellow truck'\n",
        "    sentence = [x_tk.word_index[word] for word in sentence.split()]\n",
        "    sentence = pad_sequences([sentence], maxlen=x.shape[-1], padding='post')\n",
        "    sentences = np.array([sentence[0], x[0]])\n",
        "    predictions = model.predict(sentences, len(sentences))\n",
        "\n",
        "    print('Sample 1:')\n",
        "    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[0]]))\n",
        "    print('Il a vu un vieux camion jaune')\n",
        "    print('Sample 2:')\n",
        "    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[1]]))\n",
        "    print(' '.join([y_id_to_word[np.max(x)] for x in y[0]]))\n",
        "\n",
        "\n",
        "final_predictions_GRU(preprocessed_english_sentences, preprocessed_french_sentences, english_tokenizer, french_tokenizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 110288 samples, validate on 27573 samples\n",
            "Epoch 1/80\n",
            "110288/110288 [==============================] - 23s 208us/step - loss: 1.1390 - accuracy: 0.7199 - val_loss: nan - val_accuracy: 0.8709\n",
            "Epoch 2/80\n",
            "110288/110288 [==============================] - 23s 210us/step - loss: 0.3689 - accuracy: 0.8857 - val_loss: nan - val_accuracy: 0.9271\n",
            "Epoch 3/80\n",
            "110288/110288 [==============================] - 22s 202us/step - loss: 0.2660 - accuracy: 0.9212 - val_loss: nan - val_accuracy: 0.9434\n",
            "Epoch 4/80\n",
            "110288/110288 [==============================] - 22s 201us/step - loss: 0.2273 - accuracy: 0.9338 - val_loss: nan - val_accuracy: 0.9512\n",
            "Epoch 5/80\n",
            "110288/110288 [==============================] - 22s 202us/step - loss: 0.2061 - accuracy: 0.9407 - val_loss: nan - val_accuracy: 0.9561\n",
            "Epoch 6/80\n",
            "110288/110288 [==============================] - 23s 209us/step - loss: 0.1933 - accuracy: 0.9449 - val_loss: nan - val_accuracy: 0.9577\n",
            "Epoch 7/80\n",
            "110288/110288 [==============================] - 23s 206us/step - loss: 0.1879 - accuracy: 0.9465 - val_loss: nan - val_accuracy: 0.9593\n",
            "Epoch 8/80\n",
            "110288/110288 [==============================] - 23s 208us/step - loss: 0.1786 - accuracy: 0.9490 - val_loss: nan - val_accuracy: 0.9602\n",
            "Epoch 9/80\n",
            "110288/110288 [==============================] - 23s 206us/step - loss: 0.1722 - accuracy: 0.9511 - val_loss: nan - val_accuracy: 0.9630\n",
            "Epoch 10/80\n",
            "110288/110288 [==============================] - 22s 200us/step - loss: 0.1672 - accuracy: 0.9528 - val_loss: nan - val_accuracy: 0.9626\n",
            "Epoch 11/80\n",
            "110288/110288 [==============================] - 22s 204us/step - loss: 0.1680 - accuracy: 0.9525 - val_loss: nan - val_accuracy: 0.9636\n",
            "Epoch 12/80\n",
            "110288/110288 [==============================] - 23s 207us/step - loss: 0.1666 - accuracy: 0.9528 - val_loss: nan - val_accuracy: 0.9637\n",
            "Epoch 13/80\n",
            "110288/110288 [==============================] - 22s 202us/step - loss: 0.1633 - accuracy: 0.9540 - val_loss: nan - val_accuracy: 0.9616\n",
            "Epoch 14/80\n",
            "110288/110288 [==============================] - 22s 200us/step - loss: 0.1637 - accuracy: 0.9539 - val_loss: nan - val_accuracy: 0.9649\n",
            "Epoch 15/80\n",
            "110288/110288 [==============================] - 22s 200us/step - loss: 0.1584 - accuracy: 0.9552 - val_loss: nan - val_accuracy: 0.9626\n",
            "Epoch 16/80\n",
            "110288/110288 [==============================] - 23s 206us/step - loss: 0.1638 - accuracy: 0.9539 - val_loss: nan - val_accuracy: 0.9603\n",
            "Epoch 17/80\n",
            "110288/110288 [==============================] - 23s 205us/step - loss: 0.1704 - accuracy: 0.9520 - val_loss: nan - val_accuracy: 0.9609\n",
            "Epoch 18/80\n",
            "110288/110288 [==============================] - 23s 205us/step - loss: 0.1674 - accuracy: 0.9529 - val_loss: nan - val_accuracy: 0.9630\n",
            "Epoch 19/80\n",
            "110288/110288 [==============================] - 22s 200us/step - loss: 0.1743 - accuracy: 0.9512 - val_loss: nan - val_accuracy: 0.9639\n",
            "Epoch 20/80\n",
            "110288/110288 [==============================] - 22s 201us/step - loss: 0.1617 - accuracy: 0.9548 - val_loss: nan - val_accuracy: 0.9643\n",
            "Epoch 21/80\n",
            "110288/110288 [==============================] - 22s 201us/step - loss: 0.1576 - accuracy: 0.9561 - val_loss: nan - val_accuracy: 0.9632\n",
            "Epoch 22/80\n",
            "110288/110288 [==============================] - 22s 204us/step - loss: 0.1567 - accuracy: 0.9562 - val_loss: nan - val_accuracy: 0.9643\n",
            "Epoch 23/80\n",
            "110288/110288 [==============================] - 22s 203us/step - loss: 0.1679 - accuracy: 0.9530 - val_loss: nan - val_accuracy: 0.9623\n",
            "Epoch 24/80\n",
            "110288/110288 [==============================] - 23s 205us/step - loss: 0.1608 - accuracy: 0.9553 - val_loss: nan - val_accuracy: 0.9647\n",
            "Epoch 25/80\n",
            "110288/110288 [==============================] - 23s 206us/step - loss: 0.1588 - accuracy: 0.9557 - val_loss: nan - val_accuracy: 0.9641\n",
            "Epoch 26/80\n",
            "110288/110288 [==============================] - 23s 208us/step - loss: 0.1660 - accuracy: 0.9538 - val_loss: nan - val_accuracy: 0.9621\n",
            "Epoch 27/80\n",
            "110288/110288 [==============================] - 22s 198us/step - loss: 0.1842 - accuracy: 0.9489 - val_loss: nan - val_accuracy: 0.9549\n",
            "Epoch 28/80\n",
            "110288/110288 [==============================] - 22s 203us/step - loss: 0.1751 - accuracy: 0.9513 - val_loss: nan - val_accuracy: 0.9621\n",
            "Epoch 29/80\n",
            "110288/110288 [==============================] - 22s 201us/step - loss: 0.1690 - accuracy: 0.9528 - val_loss: nan - val_accuracy: 0.9615\n",
            "Epoch 30/80\n",
            "110288/110288 [==============================] - 22s 199us/step - loss: 0.1843 - accuracy: 0.9483 - val_loss: nan - val_accuracy: 0.9583\n",
            "Epoch 31/80\n",
            "110288/110288 [==============================] - 22s 203us/step - loss: 0.1933 - accuracy: 0.9458 - val_loss: nan - val_accuracy: 0.9562\n",
            "Epoch 32/80\n",
            "110288/110288 [==============================] - 22s 201us/step - loss: 0.1950 - accuracy: 0.9454 - val_loss: nan - val_accuracy: 0.9572\n",
            "Epoch 33/80\n",
            "110288/110288 [==============================] - 23s 204us/step - loss: 0.1829 - accuracy: 0.9493 - val_loss: nan - val_accuracy: 0.9571\n",
            "Epoch 34/80\n",
            "110288/110288 [==============================] - 23s 212us/step - loss: 0.1897 - accuracy: 0.9475 - val_loss: nan - val_accuracy: 0.9586\n",
            "Epoch 35/80\n",
            "110288/110288 [==============================] - 23s 212us/step - loss: 0.1811 - accuracy: 0.9500 - val_loss: nan - val_accuracy: 0.9564\n",
            "Epoch 36/80\n",
            "110288/110288 [==============================] - 23s 208us/step - loss: 0.1959 - accuracy: 0.9457 - val_loss: nan - val_accuracy: 0.9593\n",
            "Epoch 37/80\n",
            "110288/110288 [==============================] - 22s 203us/step - loss: 0.1857 - accuracy: 0.9482 - val_loss: nan - val_accuracy: 0.9579\n",
            "Epoch 38/80\n",
            "110288/110288 [==============================] - 23s 209us/step - loss: 0.1872 - accuracy: 0.9474 - val_loss: nan - val_accuracy: 0.9572\n",
            "Epoch 39/80\n",
            "110288/110288 [==============================] - 23s 212us/step - loss: 0.1872 - accuracy: 0.9476 - val_loss: nan - val_accuracy: 0.9555\n",
            "Epoch 40/80\n",
            "110288/110288 [==============================] - 23s 213us/step - loss: 0.1865 - accuracy: 0.9481 - val_loss: nan - val_accuracy: 0.9583\n",
            "Epoch 41/80\n",
            "110288/110288 [==============================] - 23s 206us/step - loss: 0.1857 - accuracy: 0.9483 - val_loss: nan - val_accuracy: 0.9561\n",
            "Epoch 42/80\n",
            "110288/110288 [==============================] - 24s 217us/step - loss: 0.1923 - accuracy: 0.9464 - val_loss: nan - val_accuracy: 0.9576\n",
            "Epoch 43/80\n",
            "110288/110288 [==============================] - 23s 208us/step - loss: 0.2043 - accuracy: 0.9431 - val_loss: nan - val_accuracy: 0.9522\n",
            "Epoch 44/80\n",
            "110288/110288 [==============================] - 23s 208us/step - loss: 0.2050 - accuracy: 0.9430 - val_loss: nan - val_accuracy: 0.9529\n",
            "Epoch 45/80\n",
            "110288/110288 [==============================] - 23s 208us/step - loss: 0.1987 - accuracy: 0.9448 - val_loss: nan - val_accuracy: 0.9530\n",
            "Epoch 46/80\n",
            "110288/110288 [==============================] - 24s 213us/step - loss: 0.2114 - accuracy: 0.9412 - val_loss: nan - val_accuracy: 0.9525\n",
            "Epoch 47/80\n",
            "110288/110288 [==============================] - 22s 201us/step - loss: 0.2116 - accuracy: 0.9412 - val_loss: nan - val_accuracy: 0.9508\n",
            "Epoch 48/80\n",
            "110288/110288 [==============================] - 23s 204us/step - loss: 0.2195 - accuracy: 0.9383 - val_loss: nan - val_accuracy: 0.9466\n",
            "Epoch 49/80\n",
            "110288/110288 [==============================] - 24s 215us/step - loss: 0.2284 - accuracy: 0.9356 - val_loss: nan - val_accuracy: 0.9423\n",
            "Epoch 50/80\n",
            "110288/110288 [==============================] - 23s 212us/step - loss: 0.2325 - accuracy: 0.9346 - val_loss: nan - val_accuracy: 0.9501\n",
            "Epoch 51/80\n",
            "110288/110288 [==============================] - 23s 209us/step - loss: 0.2222 - accuracy: 0.9373 - val_loss: nan - val_accuracy: 0.9457\n",
            "Epoch 52/80\n",
            "110288/110288 [==============================] - 23s 211us/step - loss: 0.2330 - accuracy: 0.9346 - val_loss: nan - val_accuracy: 0.9475\n",
            "Epoch 53/80\n",
            "110288/110288 [==============================] - 23s 212us/step - loss: 0.2326 - accuracy: 0.9346 - val_loss: nan - val_accuracy: 0.9472\n",
            "Epoch 54/80\n",
            "110288/110288 [==============================] - 23s 212us/step - loss: 0.2441 - accuracy: 0.9305 - val_loss: nan - val_accuracy: 0.9386\n",
            "Epoch 55/80\n",
            "110288/110288 [==============================] - 24s 214us/step - loss: 0.2644 - accuracy: 0.9248 - val_loss: nan - val_accuracy: 0.9395\n",
            "Epoch 56/80\n",
            "110288/110288 [==============================] - 24s 213us/step - loss: 0.2674 - accuracy: 0.9238 - val_loss: nan - val_accuracy: 0.9416\n",
            "Epoch 57/80\n",
            "110288/110288 [==============================] - 24s 215us/step - loss: 0.2575 - accuracy: 0.9261 - val_loss: nan - val_accuracy: 0.9360\n",
            "Epoch 58/80\n",
            "110288/110288 [==============================] - 23s 207us/step - loss: 0.2549 - accuracy: 0.9271 - val_loss: nan - val_accuracy: 0.9421\n",
            "Epoch 59/80\n",
            "110288/110288 [==============================] - 23s 207us/step - loss: 0.2574 - accuracy: 0.9265 - val_loss: nan - val_accuracy: 0.9389\n",
            "Epoch 60/80\n",
            "110288/110288 [==============================] - 24s 215us/step - loss: 0.2607 - accuracy: 0.9253 - val_loss: nan - val_accuracy: 0.9423\n",
            "Epoch 61/80\n",
            "110288/110288 [==============================] - 23s 210us/step - loss: 0.2390 - accuracy: 0.9324 - val_loss: nan - val_accuracy: 0.9470\n",
            "Epoch 62/80\n",
            "110288/110288 [==============================] - 23s 212us/step - loss: 0.2432 - accuracy: 0.9315 - val_loss: nan - val_accuracy: 0.9403\n",
            "Epoch 63/80\n",
            "110288/110288 [==============================] - 23s 209us/step - loss: 0.2767 - accuracy: 0.9205 - val_loss: nan - val_accuracy: 0.9309\n",
            "Epoch 64/80\n",
            "110288/110288 [==============================] - 23s 209us/step - loss: 0.2889 - accuracy: 0.9163 - val_loss: nan - val_accuracy: 0.9323\n",
            "Epoch 65/80\n",
            "110288/110288 [==============================] - 23s 211us/step - loss: 0.2960 - accuracy: 0.9142 - val_loss: nan - val_accuracy: 0.9296\n",
            "Epoch 66/80\n",
            "110288/110288 [==============================] - 23s 208us/step - loss: 0.3021 - accuracy: 0.9131 - val_loss: nan - val_accuracy: 0.9320\n",
            "Epoch 67/80\n",
            "110288/110288 [==============================] - 23s 208us/step - loss: 0.2885 - accuracy: 0.9174 - val_loss: nan - val_accuracy: 0.9333\n",
            "Epoch 68/80\n",
            "110288/110288 [==============================] - 23s 209us/step - loss: 0.2839 - accuracy: 0.9194 - val_loss: nan - val_accuracy: 0.9348\n",
            "Epoch 69/80\n",
            "110288/110288 [==============================] - 22s 203us/step - loss: 0.3124 - accuracy: 0.9096 - val_loss: nan - val_accuracy: 0.9255\n",
            "Epoch 70/80\n",
            "110288/110288 [==============================] - 23s 207us/step - loss: 0.3115 - accuracy: 0.9096 - val_loss: nan - val_accuracy: 0.9255\n",
            "Epoch 71/80\n",
            "110288/110288 [==============================] - 22s 203us/step - loss: 0.2998 - accuracy: 0.9137 - val_loss: nan - val_accuracy: 0.9271\n",
            "Epoch 72/80\n",
            "110288/110288 [==============================] - 23s 212us/step - loss: 0.3062 - accuracy: 0.9117 - val_loss: nan - val_accuracy: 0.9247\n",
            "Epoch 73/80\n",
            "110288/110288 [==============================] - 23s 212us/step - loss: 0.3264 - accuracy: 0.9038 - val_loss: nan - val_accuracy: 0.9173\n",
            "Epoch 74/80\n",
            "110288/110288 [==============================] - 23s 212us/step - loss: 0.3203 - accuracy: 0.9050 - val_loss: nan - val_accuracy: 0.9191\n",
            "Epoch 75/80\n",
            "110288/110288 [==============================] - 22s 200us/step - loss: 0.3154 - accuracy: 0.9070 - val_loss: nan - val_accuracy: 0.9235\n",
            "Epoch 76/80\n",
            "110288/110288 [==============================] - 23s 212us/step - loss: 0.3176 - accuracy: 0.9067 - val_loss: nan - val_accuracy: 0.9223\n",
            "Epoch 77/80\n",
            "110288/110288 [==============================] - 23s 207us/step - loss: 0.3156 - accuracy: 0.9072 - val_loss: nan - val_accuracy: 0.9196\n",
            "Epoch 78/80\n",
            "110288/110288 [==============================] - 23s 209us/step - loss: 0.3218 - accuracy: 0.9048 - val_loss: nan - val_accuracy: 0.9273\n",
            "Epoch 79/80\n",
            "110288/110288 [==============================] - 23s 207us/step - loss: 0.3055 - accuracy: 0.9113 - val_loss: nan - val_accuracy: 0.9267\n",
            "Epoch 80/80\n",
            "110288/110288 [==============================] - 24s 216us/step - loss: 0.2984 - accuracy: 0.9144 - val_loss: nan - val_accuracy: 0.9276\n",
            "Sample 1:\n",
            "il a vu un vieux camion jaune <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
            "Il a vu un vieux camion jaune\n",
            "Sample 2:\n",
            "new jersey est parfois calme au cours et l' automne est il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD>\n",
            "new jersey est parfois calme pendant l' automne et il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdPV_QGGf_5x",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}